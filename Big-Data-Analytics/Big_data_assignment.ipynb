{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-trKdb9onDvt",
        "outputId": "157f9e6f-3ca7-494d-babc-c78367015308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=4c73939816c6595da5c0d52714590b4057ea4e5d04f561985f6f729ccb712808\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mPzEGn32n0Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n"
      ],
      "metadata": {
        "id": "K6TrKNuznRqA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ZipExample\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "49d7o5iHnKvx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets into DataFrames\n",
        "df1 = spark.read.option(\"header\", \"true\").csv(\"/content/spam_ham_dataset.csv\")\n",
        "df2 = spark.read.option(\"header\", \"true\").csv(\"/content/spam.csv\")\n"
      ],
      "metadata": {
        "id": "g3TD15YPnheE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a row index to each DataFrame\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "df1 = df1.withColumn(\"row_index\", monotonically_increasing_id())\n",
        "df2 = df2.withColumn(\"row_index\", monotonically_increasing_id())\n"
      ],
      "metadata": {
        "id": "C6O6_qnFnlHW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the two DataFrames together\n",
        "zipped_df = df1.join(df2, \"row_index\").drop(\"row_index\")\n"
      ],
      "metadata": {
        "id": "Uhs24aIsnq5b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the zipped DataFrame\n",
        "zipped_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjsG_so-ntfa",
        "outputId": "a25b5189-dbcd-4cfd-c2bb-6cdb2ea3d228"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+----+--------------------+----+----+----+\n",
            "|                 _c0|               label|                text|           label_num|  v1|                  v2| _c2| _c3| _c4|\n",
            "+--------------------+--------------------+--------------------+--------------------+----+--------------------+----+----+----+\n",
            "|                 605|                 ham|Subject: enron me...|                NULL| ham|Go until jurong p...|NULL|NULL|NULL|\n",
            "|this is a follow ...| 4 / 3 / 00 { pre...|                NULL|                NULL| ham|Ok lar... Joking ...|NULL|NULL|NULL|\n",
            "|flow data provide...|                NULL|                NULL|                NULL|spam|Free entry in 2 a...|NULL|NULL|NULL|\n",
            "|please override p...|                NULL|                NULL|                NULL| ham|U dun say so earl...|NULL|NULL|NULL|\n",
            "|activity you can ...|                NULL|                NULL|                NULL| ham|Nah I don't think...|NULL|NULL|NULL|\n",
            "|this change is ne...|                   0|                NULL|                NULL|spam|FreeMsg Hey there...|NULL|NULL|NULL|\n",
            "|                2349|                 ham|Subject: hpl nom ...|                NULL| ham|Even my brother i...|NULL|NULL|NULL|\n",
            "|( see attached fi...|                NULL|                NULL|                NULL| ham|As per your reque...|NULL|NULL|NULL|\n",
            "|  - hplnol 09 . xls\"|                   0|                NULL|                NULL|spam|WINNER!! As a val...|NULL|NULL|NULL|\n",
            "|                3624|                 ham|Subject: neon ret...|                NULL|spam|Had your mobile 1...|NULL|NULL|NULL|\n",
            "|           ho ho ho | we ' re around t...|                NULL|                NULL| ham|I'm gonna be home...|NULL|NULL|NULL|\n",
            "|i know that this ...| and that it ' s ...| but life does go...| and that ' s wha...|spam|SIX chances to wi...|NULL|NULL|NULL|\n",
            "|on the calender t...| the retreat was ...| we ' re going to...| january 12 - 13 ...|spam|URGENT! You have ...|NULL|NULL|NULL|\n",
            "|i think we all ag...| but it can be a ...|           etc . so | brad came up wit...| ham|I've been searchi...|NULL|NULL|NULL|\n",
            "|the first option ...| where we ' d hav...|      real relaxing | but also close t...| ham|I HAVE A DATE ON ...|NULL|NULL|NULL|\n",
            "|the second option...| have dinner toge...| and then have de...| but the trade of...|spam|XXXMobileMovieClu...|NULL|NULL|NULL|\n",
            "|email me back wit...| and of course if...| preferably by th...| no complaining a...| ham|Oh k...i'm watchi...|NULL|NULL|NULL|\n",
            "|have a great week...|         great golf |      great fishing |     great shopping | ham|Eh u remember how...|NULL|NULL|NULL|\n",
            "|              bobby\"|                   0|                NULL|                NULL| ham|Fine if that��s t...|NULL|NULL|NULL|\n",
            "|                4685|                spam|Subject: photosho...|                NULL|spam|England v Macedon...|NULL|NULL|NULL|\n",
            "+--------------------+--------------------+--------------------+--------------------+----+--------------------+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "ta_GuSxInuWH"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}